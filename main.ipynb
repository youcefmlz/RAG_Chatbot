{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNdRCYo7bCTk"
   },
   "source": [
    "# üîç RAG-Based Chatbot for the University of Sheffield\n",
    "\n",
    "## üìå Overview\n",
    "\n",
    "This project implements a **Retrieval-Augmented Generation (RAG)** chatbot designed to answer **research and computer science-related queries** about **the University of Sheffield**. The system integrates **hybrid document retrieval, reranking, and LLM-based answer generation and refinement** to deliver accurate, relevant, and well-formulated responses.\n",
    "\n",
    "The chatbot is built using:\n",
    "- **Hybrid retrieval** (BM25 + Chroma vector database)  \n",
    "- **A reranker model** (FlagEmbedding-based) to improve retrieval results  \n",
    "- **Llama 3.2 (3B parameter model, FP16)** for response generation and refinement\n",
    "- **Gradio** for an interactive UI   \n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "### **1Ô∏è‚É£ User Query Processing**\n",
    "- The chatbot first checks if the question is a **general greeting or FAQ** (e.g., \"Who are you?\"). If so, it returns a **predefined response** without using retrieval.  \n",
    "- If the question is **research-related**, the system searches for relevant documents in a **preprocessed knowledge base** (University of Sheffield research data).  \n",
    "\n",
    "### **2Ô∏è‚É£ Document Retrieval and Reranking**\n",
    "- The **retrieval pipeline** consists of:\n",
    "\n",
    "  ‚úÖ **BM25 (keyword-based search)** ‚Äì for retrieving exact text matches  \n",
    "  ‚úÖ **Chroma (vector search)** ‚Äì for semantic similarity matching  \n",
    "  ‚úÖ **Ensemble Retrieval** ‚Äì combines BM25 & Chroma with equal weighting  \n",
    "  ‚úÖ **FlagReranker** ‚Äì ranks documents by relevance, documents scoring below\n",
    "   **< 0.2** are discarded to avoid hallucinations and answering irrelevant questions.\n",
    "\n",
    "### **3Ô∏è‚É£ Answer Generation (LLM-Powered)**\n",
    "- The retrieved documents are passed to **Llama 3.2 (3B-Instruct-FP16)**.  \n",
    "- The chatbot is explicitly **instructed to only answer questions based on provided context**.  \n",
    "- If no documents are retrieved, it provides a **fallback response**, ensuring it does not hallucinate answers.  \n",
    "\n",
    "### **4Ô∏è‚É£ Answer refinement**\n",
    "Once an initial answer is generated, a second pass with the LLM refines it to:\n",
    "- Improve fluency, coherence, and structure\n",
    "- Ensure the answer addresses the full scope of the question\n",
    "- Correct any vague or incomplete parts\n",
    "- Avoid including any unsupported information\n",
    "\n",
    "This is done using a special prompt that:\n",
    "- Checks the correctness of the answer\n",
    "- Revises it using only the retrieved documents\n",
    "- Returns a final polished version ready to display to the user\n",
    "\n",
    "\n",
    "### **5Ô∏è‚É£ Gradio UI for User Interaction**\n",
    "- **Gradio Chat Interface**: Users can type queries and receive responses in real time.  \n",
    "- **Accordion Display**: Allows users to view retrieved documents used in generating responses.  \n",
    "- **Streaming Response**: The chatbot outputs responses word-by-word for a more natural interaction.  \n",
    "\n",
    "### **1Ô∏è‚É£ Install Dependencies**\n",
    "```bash\n",
    "!pip install -r /content/requirements.txt\n",
    "```\n",
    "### **2Ô∏è‚É£ Set API Key for Trivy web search - For Evaluation**\n",
    "```bash\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "```\n",
    "### **3Ô∏è‚É£ Download and Serve Llama 3.2(3B instruct fp16)**\n",
    "```bash\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!nohup ollama serve > /dev/null 2>&1 &\n",
    "!ollama pull llama3.2:3b-instruct-fp16 > /dev/null 2>&1 &\n",
    "```\n",
    "### **4Ô∏è‚É£ Extract and Process University Research and general documents**\n",
    "```python\n",
    "with zipfile.ZipFile(\"path_to_data_zip_file\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"parsed_data\")\n",
    "```\n",
    "### **5Ô∏è‚É£ Run the Chatbot**\n",
    "```python\n",
    "demo.launch(share=True, debug=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-H7m4uFTKqeL"
   },
   "outputs": [],
   "source": [
    "#Install all requirements\n",
    "!pip install -r /content/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83lpHbzqLHdc"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import pickle\n",
    "import getpass\n",
    "import zipfile\n",
    "import collections\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Optional\n",
    "from langchain.schema import Document\n",
    "from FlagEmbedding import FlagReranker\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.retrievers import TFIDFRetriever\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.vectorstores import Chroma,SKLearnVectorStore\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,AIMessage\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_vUK8LQcFOS"
   },
   "outputs": [],
   "source": [
    "# Enter Trivy api key\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFOeRTdK-nfL"
   },
   "outputs": [],
   "source": [
    "#install ollama server to use it localy\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eheqjo3H-5vW"
   },
   "outputs": [],
   "source": [
    "#Run ollama server in the background\n",
    "!nohup ollama serve > /dev/null 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMIYRppyApbM"
   },
   "outputs": [],
   "source": [
    "#Pull llama3.2:3b-instruct-fp16 model\n",
    "!ollama pull llama3.2:3b-instruct-fp16 > /dev/null 2>&1 &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a21FnuLjArvI",
    "outputId": "6bf9ffcb-20b1-4386-ad55-15d1760b7080"
   },
   "outputs": [],
   "source": [
    "#List all ollama models - check if the model is available\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHgiuJ8nAxIP"
   },
   "outputs": [],
   "source": [
    "#stop ollama server\n",
    "#!pkill -f \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgoLeLyELbrz"
   },
   "outputs": [],
   "source": [
    "### Initialize LLM (Llama 3.2 - 3B Parameter Model with 16-bit floating point precision) ###\n",
    "llm = ChatOllama(model=\"llama3.2:3b-instruct-fp16\",\n",
    "                 temperature=0.8,\n",
    "                 num_predict= 2048,\n",
    "                #  top_k=60,\n",
    "                #  top_p=0.95,\n",
    "                 device_map='cuda')\n",
    "\n",
    "\n",
    "llm_json_mode = ChatOllama(model=\"llama3.2:3b-instruct-fp16\",\n",
    "                           temperature=0.2,\n",
    "                           format=\"json\",\n",
    "                           device_map='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytXOFlRrOyhj"
   },
   "outputs": [],
   "source": [
    "\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
    "    chunk_overlap=80,  # the number of characters to overlap between chunks\n",
    "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
    "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH5tFy23O7lG"
   },
   "outputs": [],
   "source": [
    "#simple function to clean the text\n",
    "def preprocess_text_minimal(text):\n",
    "    # Lowercase for consistency\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove unwanted whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Optionally, remove irrelevant patterns\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)  # Remove URLs if unnecessary\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-iGq3oO2xAy"
   },
   "outputs": [],
   "source": [
    "#Extract data from zip file\n",
    "with zipfile.ZipFile(\"/content/final_llm_parsed_data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"parsed_data\")  # Extract to a new directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ6mB1xN6dMj"
   },
   "outputs": [],
   "source": [
    "llm_parsed_folder = '/content/parsed_data/final_llm_parsed_data'\n",
    "# list all files in the folder\n",
    "docs_list = []\n",
    "for file in  os.listdir(llm_parsed_folder):\n",
    "    with open(os.path.join(llm_parsed_folder, file), 'r') as f:\n",
    "        raw_content = f.read()\n",
    "\n",
    "        # Extract the source\n",
    "        source_pattern = r\"\\[SOURCE\\]\\s*(.*?)\\s*\\[LAST UPDATED\\]\"\n",
    "        source_match = re.search(source_pattern, raw_content, re.DOTALL)\n",
    "        source = source_match.group(1) if source_match else \"\"\n",
    "\n",
    "        # Extract the content\n",
    "        content_pattern = r\"\\[CONTENT\\]\\s*(.*)\"\n",
    "        content_match = re.search(content_pattern, raw_content, re.DOTALL)\n",
    "        content = content_match.group(1) if content_match else \"\"\n",
    "        docs_list.append(LangchainDocument(page_content=content, metadata={'source': source}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApGs5w0O7C3q"
   },
   "outputs": [],
   "source": [
    "#split documents into chunks\n",
    "chunked_content = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJwt7zKsL-uS"
   },
   "outputs": [],
   "source": [
    "print(f\"number of chunks: {len(chunked_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrYaHMi0oQwg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_average_chunk_length(docs):\n",
    "    lengths = [len(doc.page_content) for doc in docs]\n",
    "    return mean(lengths)\n",
    "\n",
    "\n",
    "def get_average_word_count(docs):\n",
    "    return mean(len(doc.page_content.split()) for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jqOPmv3mo4Vd"
   },
   "outputs": [],
   "source": [
    "def plot_chunk_lengths(docs):\n",
    "    lengths = [len(doc.page_content) for doc in docs]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(lengths, bins=10, edgecolor='black')\n",
    "    plt.title(\"Distribution of Document Chunk Lengths (Characters)\")\n",
    "    plt.xlabel(\"Length (in characters)\")\n",
    "    plt.ylabel(\"Number of chunks\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_chunk_word_counts(docs):\n",
    "    word_counts = [len(doc.page_content.split()) for doc in docs]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(word_counts, bins=10, edgecolor='black')\n",
    "    plt.title(\"Distribution of Document Chunk Lengths (Words)\")\n",
    "    plt.xlabel(\"Length (in words)\")\n",
    "    plt.ylabel(\"Number of chunks\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "rHNx6TFRo7rF",
    "outputId": "518de0d9-0b15-4d14-9818-47e3ec4169ff"
   },
   "outputs": [],
   "source": [
    "get_average_chunk_length(chunked_content)\n",
    "get_average_word_count(chunked_content)\n",
    "plot_chunk_lengths(chunked_content)\n",
    "plot_chunk_word_counts(chunked_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtUQM5xZHjHR",
    "outputId": "8504a819-37d7-4a88-e2f1-e47f48faf010"
   },
   "outputs": [],
   "source": [
    "# Save extracted content to a text file\n",
    "output_text_path = \"./shef_dcs_extracted_data.txt\"\n",
    "with open(output_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in docs_list:\n",
    "        f.write(f\"Content:\\n{doc.page_content}\\n\")\n",
    "print(f\"Data saved to {output_text_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBk34MMvPJHO"
   },
   "outputs": [],
   "source": [
    "#hf embeddings:\n",
    "embedding_model_name =\"nomic-ai/nomic-embed-text-v1.5\"\n",
    "retriever_model = embedding_model_name\n",
    "\n",
    "model_kwargs = {'device': 'cuda', 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "# Create the HuggingFaceEmbeddings object configuration\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C56hDVH6TyVv"
   },
   "outputs": [],
   "source": [
    "# use Chroma vector database\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunked_content,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=hf,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbncRxEBT0pM"
   },
   "outputs": [],
   "source": [
    "\n",
    "#hybrid retrieval : BM25 captures exact keyword matches + chroma captures semantic meaning\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    chunked_content\n",
    ")\n",
    "bm25_retriever.k = 10\n",
    "\n",
    "chroma_retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs={\"k\": 10}    #number of docs to retrieve\n",
    ")\n",
    "\n",
    "# initialize the ensemble retriever\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, chroma_retriever], weights=[0.5, 0.5]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "789e4ff159b247ba872225c78070860f",
      "1452f1c195ec422f9b779c4480548d34",
      "ab456cc4c5734dd7bfdb336e9b29d68a",
      "69895c3cc6624ea8b196c79546851795",
      "e11d4a87987e421f835799a4a74a53cc",
      "72d232914c384ecc94de25e63669cdd6",
      "0d8a1d09d18944d7b052306040df5672",
      "715328e917414a50bf0bfb454e93973f",
      "1ffe35412019430b97cb8e72e9dea94c",
      "6d36e2e2cdb6461c807a51a27aec97d9",
      "c87ba24c07064377abbaa8e1d48526ef",
      "f9784a4a12804aa2970d04f5a83beb35",
      "bfcb16b2ca0443429b339e403864f574",
      "a4c5c08511054a2cad54aea501dc66cd",
      "a252732725de4914a51f9bb26e74103a",
      "d9c5aa0d6d0844c7b9c1cdac992ca26d",
      "d621c9bce43d43b08573d3d5b2efc5cd",
      "17e1df1385544fdcbb9125bd004baa06",
      "a80b3e4afb3346aca163ad0e76661029",
      "0aed707dd4944b62bfb17a1925dd0c66",
      "fbb2fcc2dd4c4c72ae6360e8093a01c5",
      "81e8565f72df44e8abf67abf9ed365fc",
      "92f82e4114504b1194c539cb14b6a8f3",
      "b968d057edf94ea18aa29973abec7079",
      "7d45245e7bed43628dba836c09f15b73",
      "672d0f1970924c35b533aba48e84e400",
      "95b58676fcb54cca9e77b7cb9cfc5d2c",
      "97791b6066f746d9bed4962d126821d7",
      "e460191fa02a4b6bb73d6a1ab57aa1cb",
      "322a06e0c9754b948ebc6f1534b424c6",
      "12e54848034340f3a8f1bd0f994caa55",
      "52a81688168e431eb024aee731921ce6",
      "0891ae99040245da8f2b58c17e703ec9",
      "369ae719f4e7441bb5d71ac17e828edc",
      "c818ac2843f642d8925cf4c0c2d8fb67",
      "3229a5249ba4494891b16857c609f980",
      "456f3a9311944b8689297f6e678ae275",
      "e3d4c830cfb641588d7126a303eaa1b9",
      "224a5c9e211a484099a980d6a3d51b21",
      "706be3c915084bee9c459def9f9f70ca",
      "4a823865ac61481c893852e01d170522",
      "9f214e13e9c245dc939d56d7991dd2f8",
      "820b655b9aeb4820ac9d11a77275c704",
      "3b79b76d32e84b098bf2d9629cb719d3",
      "fbc013a9082048ef91bccb398d870aab",
      "1c4008674e7644bc870f0f4e11bdee62",
      "fe29d66e75dd45da842f1844b040f549",
      "fb4183b6c6ad4c42b78049271c5b016f",
      "58da8453d82b4e0bac702d226a7d6c0c",
      "124db3ee98e446ddb909b9127f80431c",
      "0eb14550a11e406ca6790a02be20b075",
      "219830c1f1484f31b3f3eecb244f724d",
      "648bfeda90a14e8498981800de6d5239",
      "988371221d90453d9dbbac863e606120",
      "aed594606586451fbcf10dc82e363730",
      "c7263dec7b5f47688ffee392f94de2f7",
      "b6dea86cf549428b83902925bbc97c8c",
      "0a511607892c40398ed456896c333bae",
      "47f3cc9bf972402695c5dca0f4467820",
      "f7924980699f4616a7f9b9a541557ab7",
      "93de1e486f164620b774f5edd8dda6eb",
      "5a416bcffa7a4a5382d1e65cc842e223",
      "b5bcd27e03e94b5e9c1da5f08e6d094f",
      "e7a653a28d2441febffd5dfa245aed95",
      "c578897fdd0e435699b41112d22faef4",
      "d3615f87ad21497ab5aa5b8013b2a35b"
     ]
    },
    "id": "bB2W6QEcT2MD",
    "outputId": "8c37d7b0-0205-47df-9541-d22e94d756b6"
   },
   "outputs": [],
   "source": [
    "#create a reranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpQjA2PadicV"
   },
   "outputs": [],
   "source": [
    "def web_search(question):\n",
    "    \"\"\"Performs a web search using the TavilySearchResults tool and retrieves the top k results.\n",
    "\n",
    "    Args:\n",
    "        question (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing a single string where the retrieved web results are concatenated with newline separators.\n",
    "\n",
    "    Function Details:\n",
    "    - Uses the TavilySearchResults tool to fetch the top 3 search results (`k=3`).\n",
    "    - Extracts the content from each retrieved document.\n",
    "    - Joins the content of all retrieved documents into a single string, separated by newlines.\n",
    "    - Returns the concatenated results inside a list.\n",
    "    \"\"\"\n",
    "\n",
    "    web_search_tool = TavilySearchResults(k=3)\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    # web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    return [doc[\"content\"] for doc in docs[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGhg9x7Z8mI6"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6VzsrEues0n"
   },
   "outputs": [],
   "source": [
    "# rewriting_llm= ChatOllama(model=\"llama3.2:3b-instruct-fp16\",\n",
    "#                  temperature=0.2,\n",
    "#                  num_predict= 2048,)\n",
    "\n",
    "# def rewrite_query(query):\n",
    "#     \"\"\"re write user query \"\"\"\n",
    "\n",
    "#     query_rewrite_instructions =\"\"\"You are an AI assistant tasked with reformulating user queries related to research, computer science, computer science departement at Sheffield University to improve retrieval in a RAG system.\n",
    "#     Given the original query, rewrite it to be more specific, detailed,  and likely to retrieve relevant information.\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     query_rewrite_prompt = \"\"\"Here is the original query:{question}.\n",
    "\n",
    "#     Generate Only one query without mentioning any explanations.\n",
    "\n",
    "#     Rewritten query:\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Format the prompt with actual data\n",
    "#     query_rewrite_prompt_formatted = query_rewrite_prompt.format(\n",
    "#         question=query\n",
    "#     )\n",
    "\n",
    "#     result = rewriting_llm.invoke(\n",
    "#         [SystemMessage(content=query_rewrite_instructions)]\n",
    "#         + [HumanMessage(content=query_rewrite_prompt_formatted)]\n",
    "#     )\n",
    "\n",
    "#     return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e0Zko5uT5vV"
   },
   "outputs": [],
   "source": [
    "def retrieval_grader(query, retrieved_documents):\n",
    "    \"\"\"Evaluates the relevance of retrieved documents using an LLM grader (score 0-5).\"\"\"\n",
    "\n",
    "    doc_grader_instructions = \"\"\"You are an expert evaluator assessing how relevant a set of retrieved documents are for answering a user's question.\n",
    "\n",
    "    ## Scoring Guidelines - Assign a relevance score from 0 to 5-based on the following criteria:\n",
    "    - 5: All documents are highly relevant and clearly help answer the question directly.\n",
    "    - 4: Most documents are helpful, but 1 may be only somewhat related.\n",
    "    - 3: Some documents are somewhat relevant, but others are off-topic or vague.\n",
    "    - 2: Only one document seems weakly related; the rest are irrelevant or noisy.\n",
    "    - 1: All documents are irrelevant or entirely unrelated to the question.\n",
    "    - 0: The documents are completely unrelated or confusing or No retrieved documents found at all\n",
    "\n",
    "    Return JSON with a single key 'relevance_score' containing an integer from 0 to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    doc_grader_prompt = \"\"\"Here is the user question:\\n {question}\\n.\n",
    "\n",
    "    Here is the retrieved document: \\n{retrieved_documents} \\n\n",
    "\n",
    "    Be Hard on assessing the relevance of the documents to the question\n",
    "\n",
    "    Return JSON with a single key 'relevance_score' containing a value from 0 to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt with actual data\n",
    "    doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "        retrieved_documents=retrieved_documents, question=query\n",
    "    )\n",
    "\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=doc_grader_instructions)]\n",
    "        + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "    )\n",
    "\n",
    "    return json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x45MvN5T7Yl"
   },
   "outputs": [],
   "source": [
    "def hallucination_grader(question, facts , generated_answer):\n",
    "    \"\"\"Evaluates the factual accuracy of a generated answer based on retrieved documents (hallucination score 1-3).\"\"\"\n",
    "\n",
    "    hallucination_grader_instructions = \"\"\"\n",
    "    You are a fact-checking expert evaluating whether an AI-generated response is factually grounded in the provided FACTS.\n",
    "\n",
    "    **Grading Scale (Hallucination Level):**\n",
    "    - `1`: Severe Hallucination ‚Üí The response contains **major inaccuracies** or is **mostly made up**.\n",
    "    - `2`: Moderate Hallucination ‚Üí The response is **partially correct** but contains **some inaccuracies**.\n",
    "    - `3`: No Hallucination ‚Üí The response is **fully supported** by the provided FACTS.\n",
    "\n",
    "    **Explanation:**\n",
    "    - Reference **specific parts of the FACTS** that support or contradict the response.\n",
    "    - Detail **why you assigned the given hallucination score**.\n",
    "    - Identify **any false or misleading statements** in the generated response.\n",
    "\n",
    "    **Your response should be in JSON format:**\n",
    "    {{\n",
    "        \"hallucination_score\": (1-3),\n",
    "        \"explanation\": \"Detailed reasoning for the score.\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    hallucination_grader_prompt = f\"\"\"\n",
    "\n",
    "    #Question:\n",
    "    {question}\n",
    "\n",
    "    ### FACTS:\n",
    "    {facts}\n",
    "\n",
    "    ### AI-Generated Answer:\n",
    "    {generated_answer}\n",
    "\n",
    "    **Return a JSON response with `hallucination_score` (1-3) and `explanation`.**\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke the LLM to grade hallucinations\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_prompt)]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Attempt to parse JSON\n",
    "        return json.loads(result.content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        print(\"LLM response:\", result.content)  # Print problematic response\n",
    "        return {\"hallucination_score\": 0, \"explanation\": \"Error parsing JSON response from LLM.\"} #returning default values when it fails to decode LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXZ5Zvh7VMUi"
   },
   "outputs": [],
   "source": [
    "\n",
    "def response_quality_evaluator(question, generated_answer, expected_answer):\n",
    "    \"\"\"Evaluates response quality based on Coherence, Fluency, and Relevance using an LLM grader (scores 1-5).\"\"\"\n",
    "\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an evaluator assessing the quality of an AI-generated response based on three key criteria:\n",
    "\n",
    "    **Coherence (Logical Flow and Sense-Making)**\n",
    "    - **5 (Excellent):** The response is perfectly coherent, well-organized, and easy to follow. Ideas connect seamlessly with no confusion.\n",
    "    - **4 (Good):** The response is clear and logical with only minor flaws in coherence or flow. The information is well-structured.\n",
    "    - **3 (Acceptable):** The response generally makes sense but may have minor logical gaps or require extra effort to understand.\n",
    "    - **2 (Fair):** The response shows some logic, but the ideas are poorly organized or difficult to follow. Key points may be missing or muddled.\n",
    "    - **1 (Poor):** The response is completely incoherent, illogical, or nonsensical. The text is confusing and fails to convey any meaningful information.\n",
    "\n",
    "    **Fluency (Language Clarity and Naturalness)**\n",
    "    - **5 (Excellent):** The language is highly fluent, natural, and engaging. The text is polished and professional.\n",
    "    - **4 (Good):** The language is fluent and natural with few or no grammatical issues. The text reads smoothly.\n",
    "    - **3 (Acceptable):** The language is mostly clear, but some awkward phrasing or minor errors are present.\n",
    "    - **2 (Fair):** The language is somewhat unnatural or has notable grammatical errors. The response is hard to read in some parts.\n",
    "    - **1 (Poor):** The language is awkward, unnatural, or difficult to read. Multiple grammatical errors severely hinder comprehension.\n",
    "\n",
    "    **Relevance (Alignment with User's Query)**\n",
    "    - **5 (Excellent):** The response is fully relevant, highly informative, and directly addresses the query with precision.\n",
    "    - **4 (Good):** The response is relevant, accurate, and adequately detailed. Minor improvements may enhance precision.\n",
    "    - **3 (Acceptable):** The response is mostly relevant but may omit key details or include minor irrelevant content.\n",
    "    - **2 (Fair):** The response is only partially relevant, with significant off-topic content or inaccuracies.\n",
    "    - **1 (Poor):** The response is completely irrelevant or unrelated to the query. It provides no useful information.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Question:\n",
    "    {question}\n",
    "\n",
    "    ### Expected Answer:\n",
    "    {expected_answer}\n",
    "\n",
    "    ### AI-Generated Answer:\n",
    "    {generated_answer}\n",
    "\n",
    "    ### **Your Task:**\n",
    "    Carefully evaluate the AI-generated answer **based on the above criteria** use the expected answer as a comparison answer. Assign a **numerical score (1-5)** for each category.\n",
    "\n",
    "    **Provide a JSON response with the following structure:**\n",
    "    {{\n",
    "        \"coherence\": (1-5),\n",
    "        \"fluency\": (1-5),\n",
    "        \"relevance\": (1-5)\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Invoke the LLM to grade the response\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=\"You are an expert evaluator.\")]+\n",
    "        [HumanMessage(content=evaluation_prompt)]\n",
    "    )\n",
    "\n",
    "    return json.loads(result.content)  # Convert LLM response into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0AjOzQ8T8wv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def answer_grader(question, generated_answer, expected_answer, retrieved_docs=None):\n",
    "    \"\"\"Grades the generated answer on a scale of 1 to 5 based on correctness and hallucinations.\"\"\"\n",
    "\n",
    "    answer_grader_instructions = \"\"\"You are a grader assessing the accuracy of a generated answer to a user question.\n",
    "\n",
    "    **Grading Criteria (Score from 1 to 5):**\n",
    "    - `5`: Fully correct, no hallucinations, and complete.\n",
    "    - `4`: Mostly correct, minor inaccuracies but still relevant.\n",
    "    - `3`: Partially correct, some key facts missing or slightly incorrect.\n",
    "    - `2`: Incorrect or contains significant hallucinations.\n",
    "    - `1`: Completely wrong or entirely made up.\n",
    "\n",
    "    **Considerations:**\n",
    "    - Compare the generated answer to the expected answer.\n",
    "    - If retrieved documents are provided, ensure the generated answer is supported by them.\n",
    "    - Flag hallucinations if the answer includes details **not present in the retrieved documents**.\n",
    "    - If the generated answer does not answer the question or say I dont know you should give it a low score.\n",
    "\n",
    "    **Return a JSON object with:**\n",
    "    - `answer_score`: An integer from 1 to 5.\n",
    "    - `explanation`: A brief justification for the score.\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieved_texts = [doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in retrieved_docs]\n",
    "    retrieved_context = \"\\n\\n\".join(retrieved_docs) if retrieved_docs else \"No retrieved documents provided.\"\n",
    "\n",
    "    answer_grader_prompt = f\"\"\"QUESTION: \\n\\n{question}\\n\\n\n",
    "    EXPECTED ANSWER: \\n\\n{expected_answer}\\n\\n\n",
    "    GENERATED ANSWER: \\n\\n{generated_answer}\\n\\n\n",
    "    RETRIEVED DOCUMENTS:\\n\\n{retrieved_context}\\n\\n\n",
    "\n",
    "    **Return a JSON object with `answer_score` (1-5) and `explanation` fields.**\"\"\"\n",
    "\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=answer_grader_instructions)]\n",
    "        + [HumanMessage(content=answer_grader_prompt)]\n",
    "    )\n",
    "\n",
    "    return json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GL5V3LFhdav"
   },
   "outputs": [],
   "source": [
    "\n",
    "def answer_review(question, generated_answer, retrieved_context=None):\n",
    "    \"\"\"Grades the generated answer on a scale of 1 to 5 based on correctness and hallucinations.\"\"\"\n",
    "\n",
    "    answer_grader_instructions = \"\"\"You are a grader assessing the accuracy and coherence of the generated answer.\n",
    "\n",
    "    **Grading Criteria (Score from 1 to 5):**\n",
    "    - `5`: Fully correct, no hallucinations, and perfectly complete and well-organized.\n",
    "    - `4`: Mostly correct answer, minor inaccuracies but still relevant, clear and logical with only minor flaws in coherence .\n",
    "    - `3`: Partially correct does not answer all the question, some key facts missing or slightly incorrect.\n",
    "    - `2`: Incorrect or contains significant hallucinations and the ideas are poorly organized or difficult to follow.\n",
    "    - `1`: Completely wrong or entirely made up and incoherent, illogical, or nonsensical.\n",
    "\n",
    "    **Considerations:**\n",
    "    - give low score if the generated answer is not relevant to the question.\n",
    "    - give low score if the generated answer is NOt supported by the retrieved documents.\n",
    "    - give a low score if the generated answer DOES NOT answer the question.\n",
    "\n",
    "    **Return a JSON object with:**\n",
    "    - `answer_score`: An integer from 1 to 5.\n",
    "    - `explanation`: A brief justification for the score.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_grader_prompt = f\"\"\"QUESTION: \\n\\n{question}\\n\\n\n",
    "    GENERATED ANSWER: \\n\\n{generated_answer}\\n\\n\n",
    "    RETRIEVED DOCUMENTS:\\n\\n{retrieved_context}\\n\\n\n",
    "\n",
    "    **Return a JSON object with `answer_score` (1-5) and `explanation` fields.**\"\"\"\n",
    "\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=answer_grader_instructions)]\n",
    "        + [HumanMessage(content=answer_grader_prompt)]\n",
    "    )\n",
    "\n",
    "    return json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZbfT4DDheAl"
   },
   "outputs": [],
   "source": [
    "def refine_answer(question, retrieved_docs, generated_answer):\n",
    "    \"\"\"\n",
    "    Reviews and rewrites a generated answer based on the question and retrieved context.\n",
    "    Ensures the rewritten answer is accurate, context-grounded, and more fluent.\n",
    "    \"\"\"\n",
    "\n",
    "    review_and_rewrite_instructions = \"\"\"\n",
    "You are an assistant that improves chatbot answers by first evaluating the answer for correctness and relevancy,\n",
    "and then rewriting it using the RETRIEVED CONTEXT given to be more accurate, answer the whole question,fluent, and coherent.\n",
    "\n",
    "### Your job:\n",
    "1. Make sure the answer addresses the question based ONLY on the retrieved context.\n",
    "2. If the answer is incorrect, incomplete, or unrelated, revise it using only the context given.\n",
    "3. If the answer is correct, rephrase it to improve fluency, structure, and clarity.\n",
    "4. NEVER introduce information that is not present in the context.\n",
    "\n",
    "### Output format:\n",
    "Return JUST the revised answer as plain text\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"QUESTION:\n",
    "{question}\n",
    "\n",
    "GENERATED ANSWER:\n",
    "{generated_answer}\n",
    "\n",
    "RETRIEVED CONTEXT:\n",
    "{retrieved_docs}\n",
    "\n",
    "### Output format:\n",
    "Return ONLY the revised answer as plain text.\n",
    "\"\"\"\n",
    "\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=review_and_rewrite_instructions)] +\n",
    "        [HumanMessage(content=prompt)]\n",
    "    )\n",
    "\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr9p_i5zUCeI"
   },
   "outputs": [],
   "source": [
    "# Define Retrieval Function\n",
    "def retrieve_context(question):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for the given question.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "      #Retrieve documents based on the question\n",
    "      docs = retriever.get_relevant_documents(question)\n",
    "      #no documents retrieved return empty string\n",
    "      if (len(docs) == 0 ):\n",
    "          print(\"unable to find matching documents\")\n",
    "          return \"\"\n",
    "\n",
    "      #Rerank retrieved documents based on their relevance score - >= threshold(score=0.5)\n",
    "      scores = [reranker.compute_score([question, doc.page_content], normalize=True)[0] for doc in docs]\n",
    "      ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "      print(f\"scores : {ranked_docs}\")\n",
    "      filtered_docs = [doc for doc, score in ranked_docs if score>0.2]\n",
    "\n",
    "      # Keep only the top 3 high-scoring documents\n",
    "      return filtered_docs[:3]\n",
    "    except:\n",
    "      print(\"unable to find matching documents\")\n",
    "      return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFtIPy87DBog"
   },
   "outputs": [],
   "source": [
    "#format docs to one string seperated by new lines\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"{doc.page_content}\\nSource: {doc.metadata.get('source', 'Unknown')}\" for doc in docs),\"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def generate_answer_llama(question , docs):\n",
    "  #Prompt used to generate answer\n",
    "  rag_prompt = \"\"\"You are an assistant for answering questions about research and computer science at the university of Sheffield . Answer the user's question based **only** on the provided context. Please follow the instructions:\n",
    "\n",
    "  ### Instructions:\n",
    "  - Answer the question based on the given context.\n",
    "\n",
    "  - Answer the question in a natural coherent way.\n",
    "\n",
    "  - Provide a **clear, precise, and complete** answer to the question without additional explanations and information.\n",
    "\n",
    "  - Ensure your response is **well-structured and informative**.\n",
    "\n",
    "  - Make sure to use all the context that is relevant to the question.\n",
    "\n",
    "\n",
    "  ##Context: {context}\n",
    "\n",
    "  ##Question: {question}\n",
    "\n",
    "  ##Answer:\"\"\"\n",
    "\n",
    "  docs_txt_with_sources,docs_txt = format_docs(docs)\n",
    "  #template the prompt with the docs and asked question\n",
    "  rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "  #Ask the LLM\n",
    "  generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)]).content\n",
    "  rephrased_answer = refine_answer(question , docs_txt , generation)\n",
    "  return rephrased_answer , docs_txt, docs_txt_with_sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp9Ta3xm-Yvn"
   },
   "outputs": [],
   "source": [
    "#Handle general questions, without using RAG pipeline\n",
    "\n",
    "GENERAL_QUESTIONS = {\n",
    "    r\"hi+i*|hey+|hola+\": \"Hello! How can I assist you today?\",\n",
    "    r\"hello+\": \"Hi there! What can I help you with?\",\n",
    "    r\"how are you\\s*\\??\": \"I'm just a bot, but I'm here to help!\",\n",
    "    r\"who are you\\s*\\??\": \"I am an AI assistant designed to answer questions about research at the University of Sheffield.\",\n",
    "    r\"what can you do\\s*\\??\": \"I can answer research-related questions about the University of Sheffield. Feel free to ask!\"\n",
    "}\n",
    "\n",
    "def is_general_question(question: str):\n",
    "    \"\"\"Check if the input question is a general query and return a response if it is.\"\"\"\n",
    "    question_lower = question.lower().strip()\n",
    "\n",
    "    # Check if the question matches a predefined response\n",
    "    for key, response in GENERAL_QUESTIONS.items():\n",
    "        if re.match(rf\"\\b{key}\\b\", question_lower):\n",
    "            return response\n",
    "\n",
    "    return None  # Not a general question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dzo3WcXAx-Bd"
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_input( question):\n",
    "    \"\"\"Retrieves documents, generates an answer\"\"\"\n",
    "    # Check for general questions first. return answer without using RAG pipeline\n",
    "    general_response = is_general_question(question)\n",
    "    if general_response is not None:\n",
    "        return general_response, \"No documents retrieved.\"\n",
    "\n",
    "    # Retrieve Documents\n",
    "    retrieved_docs = retrieve_context(question)\n",
    "\n",
    "    if (len(retrieved_docs) == 0 ):\n",
    "        return (\n",
    "            \"I'm a chatbot designed to answer questions about the University of Sheffield. \"\n",
    "            \"Please ask a relevant question related to this topic.\",\n",
    "            \"No documents retrieved.\"\n",
    "        )\n",
    "\n",
    "    rag_output, retrieved_txt , docs_with_sources = generate_answer_llama(question, retrieved_docs)\n",
    "    # review = answer_review(question, rag_output, retrieved_txt)\n",
    "    # answer_score = review[\"answer_score\"]\n",
    "    # expl = review[\"explanation\"]\n",
    "    # print(f\" ********* answer score {answer_score}\")\n",
    "    # print(f\" ******    explanation of the answer score:: {expl} \\n\")\n",
    "\n",
    "    # if(answer_score < 4):\n",
    "    #   print(f\"inside if condition generated answer is : {rag_output}\")\n",
    "    #   return (\n",
    "    #       \"I‚Äôm a chatbot designed to answer questions about research in the Department of Computer Science at the University of Sheffield.\"\n",
    "    #       \" Please ask a relevant question related to this topic.\",\n",
    "    #       \"No documents retrieved.\"\n",
    "    #       )\n",
    "\n",
    "    # print(f\"ANSWER BEFORE REFINING : \\n {rag_output}\")\n",
    "    # rephrased_answer = refine_answer(question , retrieved_txt , rag_output)\n",
    "    # print(f\"ANSWER AFTER REFINING : \\n {rephrased_answer}\")\n",
    "\n",
    "    return rag_output,docs_with_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hOul95uymsUH"
   },
   "outputs": [],
   "source": [
    "# Function to process user input and return response for the UI\n",
    "def process_input_gradio_ui( message , history ):\n",
    "    \"\"\"Processes the user's question using the RAG system and returns only the latest response.\"\"\"\n",
    "\n",
    "    # Call your RAG processing function\n",
    "    rag_response ,docs_with_sources= process_input(message)\n",
    "    if (len(rag_response) == 0 ):\n",
    "      return  (\"I‚Äôm a chatbot designed to answer questions about research in the Department of Computer Science at the University of Sheffield.\"\n",
    "                \" Please ask a relevant question related to this topic.\",\n",
    "                \"No Documents retrieved\")\n",
    "\n",
    "    # Stream response word by word\n",
    "    response_text = \"\"\n",
    "    for word in rag_response.split():\n",
    "        response_text += word + \" \"\n",
    "        yield response_text, docs_with_sources  # Stream the update\n",
    "        time.sleep(0.05)  # Simulate streaming delay\n",
    "\n",
    "\n",
    "# Create Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\" <center><h1>üîç University of Sheffield Assistant</h1></center>\")\n",
    "\n",
    "\n",
    "    # Define the Accordion for displaying retrieved documents\n",
    "    with gr.Accordion(\"üìÑ Show Retrieved Documents\", open=False):\n",
    "        retrieved_docs_output = gr.Markdown()\n",
    "\n",
    "    # Initialize the ChatInterface with additional outputs\n",
    "    chat_interface = gr.ChatInterface(\n",
    "        fn=process_input_gradio_ui,stop_btn=True,\n",
    "        additional_outputs=[retrieved_docs_output],\n",
    "        type=\"messages\",autoscroll = True,editable=True,\n",
    "        save_history=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Launch the Gradio UI\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9l2NFLIgc_p"
   },
   "source": [
    "# üìä **Comprehensive Evaluation of RAG Chatbot vs. Baseline Models**\n",
    "\n",
    "## üîç **Purpose**\n",
    "This evaluation framework provides a rigorous and multifaceted assessment of a **Retrieval-Augmented Generation (RAG) chatbot** compared to several baseline models. The goal is to evaluate both **retrieval effectiveness** and **answer generation quality**, using a set of **105 manually prepared test questions and reference answers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ **Evaluation Phases**\n",
    "\n",
    "The pipeline is divided into **three major evaluation phases**:\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **1. Document Retrieval Evaluation**\n",
    "\n",
    "**Goal:** Evaluate how well each method retrieves relevant information for a given query.\n",
    "\n",
    "**Retrieval Methods Compared:**\n",
    "- **RAG Retrieval System**: Combines vector-based retrieval (Chroma), BM25, and a reranking model.\n",
    "- **TF-IDF Retrieval**: Classic sparse retrieval baseline.\n",
    "- **BM25 Retrieval**: Keyword-based statistical baseline.\n",
    "\n",
    "**Metrics:**\n",
    "- **Relevance Score (0‚Äì5):** Rated by an LLM grader based on alignment with query intent.\n",
    "- **Hit Rate (Score ‚â• 4):** Proportion of queries with high-quality document matches.\n",
    "- **Retrieval Time:** Time required to fetch the documents.\n",
    "\n",
    "**Outputs:**\n",
    "- Scores and logs written to `/content/retrieval_scores_result.txt`\n",
    "- **Contrastive Judgments** by an LLM comparing RAG vs. TF-IDF retrieval (stored in `/content/retrieval_comparison_results.txt`)\n",
    "- Visual comparisons via bar charts\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **2. Answer Generation Evaluation**\n",
    "\n",
    "**Goal:** Evaluate the quality of answers generated by different models using retrieved or external data.\n",
    "\n",
    "**Models Evaluated:**\n",
    "- **RAG Chatbot**: Uses hybrid retrieval and a LLaMA 3.2-based generation.\n",
    "- **LLM Only**: LLaMA 3.2 answering without retrieval context.\n",
    "- **Web Search Chatbot**: Leverages external web search results.\n",
    "- **TF-IDF + LLM**: Combines basic retrieval with the same LLM.\n",
    "\n",
    "**Metrics:**\n",
    "- **Answer Score (1‚Äì5):** Measures overall correctness and completeness.\n",
    "- **Coherence Score (1‚Äì5):** Evaluates clarity and logical flow.\n",
    "- **Fluency Score (1‚Äì5):** Measures grammar and readability.\n",
    "- **Relevance Score (1‚Äì5):** Assesses topical alignment with the question.\n",
    "- **Hallucination Score (1‚Äì3):** Indicates factual accuracy (3 = no hallucination).\n",
    "- **Answer Time:** Duration taken to generate a response.\n",
    "\n",
    "**Outputs:**\n",
    "- Detailed logs in `/content/answer_scores_result.txt`\n",
    "- Coherence, fluency, relevance scores in `/content/chatbot_quality_score.txt`\n",
    "- Hallucination evaluation results in `/content/hallucination_score.txt`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **3. Automated RAG Evaluation with RAGAS**\n",
    "\n",
    "**Goal:** Use standardized RAG metrics to perform structured evaluation of retrieved context and generated answers.\n",
    "\n",
    "**Datasets Evaluated:**\n",
    "- **RAG Chatbot**\n",
    "- **TF-IDF + LLM**\n",
    "- **Web Search Chatbot**\n",
    "- **LLM-Only**\n",
    "\n",
    "**Metrics Used:**\n",
    "- `context_recall`\n",
    "- `faithfulness`\n",
    "- `factual_correctness (F1)`\n",
    "- `context_precision_with_reference`\n",
    "- `context_precision_without_reference`\n",
    "\n",
    "**Outputs:**\n",
    "- Results saved as `.json` and loaded into RAGAS\n",
    "- Evaluation summaries printed and plotted\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Visualization**\n",
    "\n",
    "### üî∏ Retrieval Evaluation\n",
    "- üìä **Bar Charts**: Average relevance score, hit rate (score ‚â• 4), and retrieval time per method\n",
    "- ü§ñ **Contrastive LLM Judgments**: Count of times RAG vs. TF-IDF was preferred\n",
    "\n",
    "### üî∏ Answer Evaluation\n",
    "- üìä **Bar Charts**: Average answer score, coherence, fluency, relevance, hallucination\n",
    "- ‚ö° **Answer Time** comparison across models\n",
    "- üéØ **Hit Rates** (Score ‚â• 4) breakdown by metric\n",
    "\n",
    "### üî∏ Hallucination Analysis\n",
    "- üìâ **Average hallucination scores**\n",
    "- üìä **Score distribution histograms** per model\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇ **Evaluation Artifacts**\n",
    "\n",
    "- üìÑ `/content/retrieval_scores_result.txt` ‚Äì Relevance scores and retrieved docs  \n",
    "- üìÑ `/content/retrieval_comparison_results.txt` ‚Äì LLM judgments comparing retrieval systems  \n",
    "- üìÑ `/content/answer_scores_result.txt` ‚Äì Answer scores for each model  \n",
    "- üìÑ `/content/chatbot_quality_score.txt` ‚Äì Fluency, coherence, and relevance logs  \n",
    "- üìÑ `/content/hallucination_score.txt` ‚Äì Hallucination scores and rationales  \n",
    "- üìÅ `*.json` files ‚Äì Formatted input for RAGAS (e.g., `RAG_chat_data.json`, etc.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uy3VpuYelzXM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BSxFdDbgkpE"
   },
   "outputs": [],
   "source": [
    "## read the questions:\n",
    "with open(\"/content/questions.txt\") as file:\n",
    "  evaluation_questions=[line.strip() for line in file.readlines() ]\n",
    "## read the answers:\n",
    "with open(\"/content/answers.txt\") as file:\n",
    "  expected_answers=[line.strip() for line in file.readlines() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeZdd-iArdoL",
    "outputId": "93f76333-6bf6-4e3b-dbc4-9d462dd436df"
   },
   "outputs": [],
   "source": [
    "len(evaluation_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUIMeSH4OPWF"
   },
   "outputs": [],
   "source": [
    "# Simple function to answer a question using a LLM llama3.2:3b - Base Line model to compare with RAG chatbot\n",
    "def simple_llama_answer(question):\n",
    "  return llm.invoke([HumanMessage(content=question)]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONTw-RjfzuWY"
   },
   "outputs": [],
   "source": [
    "def chatbot_web_search_docs(question):\n",
    "\n",
    "  web_search_docs = web_search(question)\n",
    "  # Handle the case where no web search results are found\n",
    "  if not web_search_docs or web_search_docs == [\"\"]:\n",
    "      return \"I'm sorry, but I couldn't find relevant information from the web search.\", \"No web search documents found.\"\n",
    "\n",
    "  rag_prompt = \"\"\"You are an excellent assistant for answering questions about research and computer science at the university of Sheffield . Answer the user's question based **only** on the provided context.\n",
    "\n",
    "  ##Here is the context to use to answer the question. Read it attentively and use it:\n",
    "  {context}\n",
    "\n",
    "  ##Question:\n",
    "  {question}\n",
    "\n",
    "  ##Answer:\"\"\"\n",
    "\n",
    "\n",
    "  #template the prompt with the docs and asked question\n",
    "  rag_prompt_formatted = rag_prompt.format(context=web_search_docs, question=question)\n",
    "\n",
    "  #Ask the LLM\n",
    "  generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)]).content\n",
    "\n",
    "\n",
    "  return generation , web_search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbSWmekGWE03"
   },
   "outputs": [],
   "source": [
    "#### create tfidf retriever as a base line model to compare it with my retriever system\n",
    "def tfidf_docs_retriever(question):\n",
    "  tfidf_retriever = TFIDFRetriever.from_documents(chunked_content)\n",
    "  tfidf_retriever.k = 3\n",
    "  return tfidf_retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDyAUpyazMk0"
   },
   "outputs": [],
   "source": [
    "#### create bm25 retriever as a base line model to compare it with my retriever system\n",
    "def bm25_docs_retriever(question):\n",
    "  bm25_retriever = BM25Retriever.from_documents(chunked_content)\n",
    "  bm25_retriever.k = 3\n",
    "  return bm25_retriever.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO6zYzisJ3U6"
   },
   "outputs": [],
   "source": [
    "## this one retrieves documents using TFIDF\n",
    "\n",
    "def simple_generate_answer_llama(question):\n",
    "  #Prompt used to generate answer\n",
    "  rag_prompt = \"\"\"You are an assistant for answering questions about research and computer science at the university of Sheffield . Answer the user's question based **only** on the provided context.\n",
    "\n",
    "  ##Context: {context}\n",
    "\n",
    "  ##Question: {question}\n",
    "\n",
    "  ##Answer:\"\"\"\n",
    "\n",
    "  docs = tfidf_docs_retriever(question)\n",
    "  formatted_docs = \"\\n\".join(doc.page_content for doc in docs)\n",
    "  #template the prompt with the docs and asked question\n",
    "  rag_prompt_formatted = rag_prompt.format(context=formatted_docs, question=question)\n",
    "  #Ask the LLM\n",
    "  generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)]).content\n",
    "\n",
    "  return generation, formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xkvtxjUgzpJ",
    "outputId": "debc76a8-a5c1-4aaf-f4f3-fb387a964350"
   },
   "outputs": [],
   "source": [
    "### Evaluate retrieval system ###\n",
    "\n",
    "RAG_retrieval_times = []\n",
    "RAG_retrieval_score = []\n",
    "tfidf_retrieval_times = []\n",
    "tfidf_retrieval_score = []\n",
    "bm25_retrieval_times = []\n",
    "bm25_retrieval_score = []\n",
    "ALL_RAG_retrieved_docs_STRING = []\n",
    "ALL_RAG_retrieved_docs_LIST = []\n",
    "ALL_TFIDF_retrieved_docs_STRING = []\n",
    "ALL_BM25_retrieved_docs_STRING = []\n",
    "\n",
    "output_file= \"/content/retrieval_scores_result.txt\"\n",
    "with open(output_file , \"a\" , encoding=\"utf-8\") as f:\n",
    "  for question in tqdm(evaluation_questions, desc=\"Processing Questions\", unit=\"qst\"):\n",
    "\n",
    "    #eval RAG retrieval system\n",
    "    RAG_retrieval_start_time = time.time()\n",
    "    RAG_retrieved_docs = retrieve_context(question)\n",
    "    RAG_retrieval_times.append(time.time() - RAG_retrieval_start_time)\n",
    "    ALL_RAG_retrieved_docs_LIST.append([doc.page_content for doc in RAG_retrieved_docs])\n",
    "    formatted_docs = \"\\n\".join(doc.page_content for doc in RAG_retrieved_docs)\n",
    "    ALL_RAG_retrieved_docs_STRING.append(formatted_docs)\n",
    "\n",
    "    #eval simple TF-IDF retrieval system\n",
    "    tfidf_retrieval_start_time = time.time()\n",
    "    tfidf_retrieved_docs = tfidf_docs_retriever(question)\n",
    "    tfidf_retrieval_times.append(time.time() - tfidf_retrieval_start_time)\n",
    "    formatted_docs = \"\\n\".join(doc.page_content for doc in tfidf_retrieved_docs)\n",
    "    ALL_TFIDF_retrieved_docs_STRING.append(formatted_docs)\n",
    "\n",
    "    #eval simple BM25 retrieval system\n",
    "    bm25_retrieval_start_time = time.time()\n",
    "    bm25_retrieved_docs = bm25_docs_retriever(question)\n",
    "    bm25_retrieval_times.append(time.time() - bm25_retrieval_start_time)\n",
    "    formatted_docs = \"\\n\".join(doc.page_content for doc in tfidf_retrieved_docs)\n",
    "    ALL_BM25_retrieved_docs_STRING.append(formatted_docs)\n",
    "\n",
    "    ## score all three docs at once.\n",
    "    RAG_formatted_docs = \"\\n\\n\".join([doc.page_content for doc in RAG_retrieved_docs])  # Format retrieved docs\n",
    "    tfidf_formatted_docs = \"\\n\\n\".join([doc.page_content for doc in tfidf_retrieved_docs])\n",
    "    bm25_formatted_docs = \"\\n\\n\".join([doc.page_content for doc in bm25_retrieved_docs])\n",
    "\n",
    "    # Calculate the relevance scores\n",
    "    RAG_relevance_score = retrieval_grader(question, RAG_formatted_docs)[\"relevance_score\"]\n",
    "    tfidf_relevance_score = retrieval_grader(question, tfidf_formatted_docs)[\"relevance_score\"]\n",
    "    bm25_relevance_score = retrieval_grader(question, bm25_formatted_docs)[\"relevance_score\"]\n",
    "\n",
    "\n",
    "\n",
    "    ## write to the result file:\n",
    "    f.write(f\"Question: {question}\\n\")\n",
    "    f.write(f\"RAG relevance Score: {RAG_relevance_score}\\n\")\n",
    "    f.write(f\"TF-IDF relevance Score: {tfidf_relevance_score}\\n\")\n",
    "    f.write(f\"BM25 relevance Score: {bm25_relevance_score}\\n\")\n",
    "    f.write(f\"RAG retrieved Documents:\\n{RAG_formatted_docs}\\n\")\n",
    "    f.write(f\"TF-IDF retrieved Documents:\\n{tfidf_formatted_docs}\\n\")\n",
    "    f.write(f\"BM25 retrieved Documents:\\n{bm25_formatted_docs}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")  # Separator\n",
    "\n",
    "    RAG_retrieval_score.append(RAG_relevance_score)\n",
    "    tfidf_retrieval_score.append(tfidf_relevance_score)\n",
    "    bm25_retrieval_score.append(bm25_relevance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvyosiWaiL19",
    "outputId": "41b37528-f7c3-401e-f508-0e092716d232"
   },
   "outputs": [],
   "source": [
    "#### CALCULATE EVALUATION METRICS FOR RETRIEVAL (Both tf-idf and RAG chatbot). ####\n",
    "\n",
    "## Average retrieval score\n",
    "#RAG\n",
    "RAG_average_retrieval_score = sum(RAG_retrieval_score) / len(RAG_retrieval_score)\n",
    "print(f\"Average RAG Retrieval Score: {RAG_average_retrieval_score}\")\n",
    "#tfidf\n",
    "tfidf_average_retrieval_score = sum(tfidf_retrieval_score) / len(tfidf_retrieval_score)\n",
    "print(f\"Average TF-IDF Retrieval Score: {tfidf_average_retrieval_score}\")\n",
    "#BM25\n",
    "bm25_average_retrieval_score = sum(bm25_retrieval_score) / len(bm25_retrieval_score)\n",
    "print(f\"Average BM25 Retrieval Score: {bm25_average_retrieval_score}\")\n",
    "\n",
    "## SCORE DISTRIBUTION\n",
    "#RAG\n",
    "RAG_score_distribution = collections.Counter(RAG_retrieval_score)\n",
    "print(\"Score Distribution - RAG retrieval system:\", RAG_score_distribution)\n",
    "#tfidf\n",
    "tfidf_score_distribution = collections.Counter(tfidf_retrieval_score)\n",
    "print(\"Score Distribution - TF-IDF retrieval system:\", tfidf_score_distribution)\n",
    "#BM25\n",
    "bm25_score_distribution = collections.Counter(bm25_retrieval_score)\n",
    "print(\"Score Distribution - BM25 retrieval system:\", bm25_score_distribution)\n",
    "\n",
    "## CALCULATE HIT RATE >=4 :\n",
    "#RAG\n",
    "RAG_hit_rate = sum(1 for s in RAG_retrieval_score if s >= 4) / len(RAG_retrieval_score)\n",
    "print(f\"Hit Rate (Score ‚â• 4) -- RAG --: {RAG_hit_rate:.2%}\")\n",
    "#tfidf\n",
    "tfidf_hit_rate = sum(1 for s in tfidf_retrieval_score if s >= 4) / len(tfidf_retrieval_score)\n",
    "print(f\"Hit Rate (Score ‚â• 4) -- TF-IDF --: {tfidf_hit_rate:.2%}\")\n",
    "#BM25\n",
    "bm25_hit_rate = sum(1 for s in bm25_retrieval_score if s >= 4) / len(bm25_retrieval_score)\n",
    "print(f\"Hit Rate (Score ‚â• 4) -- BM25 --: {bm25_hit_rate:.2%}\")\n",
    "\n",
    "## CALCULATE AVERAGE TIME TAKEN FOR RETRIEVING THE DOCS :\n",
    "#RAG\n",
    "RAG_average_retrieval_time = sum(RAG_retrieval_times) / len(RAG_retrieval_times)\n",
    "print(f\"Average Retrieval Time -- RAG --: {RAG_average_retrieval_time:.2f} seconds\")\n",
    "#tfidf\n",
    "tfidf_average_retrieval_time = sum(tfidf_retrieval_times) / len(tfidf_retrieval_times)\n",
    "print(f\"Average Retrieval Time -- TF-IDF --: {tfidf_average_retrieval_time:.2f} seconds\")\n",
    "#BM25\n",
    "bm25_average_retrieval_time = sum(bm25_retrieval_times) / len(bm25_retrieval_times)\n",
    "print(f\"Average Retrieval Time -- BM25 --: {bm25_average_retrieval_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QcTAtrh4lrso",
    "outputId": "635255d5-1d00-41fa-9032-e9bd172fd13e"
   },
   "outputs": [],
   "source": [
    "### plot graphs for retrieval evaluation ###\n",
    "\n",
    "# Bar chart for average retrieval scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"RAG\", \"TF-IDF\",\"BM25\"], [RAG_average_retrieval_score, tfidf_average_retrieval_score,bm25_average_retrieval_score],width =0.7 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Average Retrieval Score\")\n",
    "plt.title(\"Average Retrieval Score Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for hit rates\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"RAG\", \"TF-IDF\",\"BM25\"], [RAG_hit_rate, tfidf_hit_rate,bm25_hit_rate], width=0.7 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Hit Rate (Score ‚â• 4)\")\n",
    "plt.title(\"Hit Rate Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for average retrieval times\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"RAG\", \"TF-IDF\",\"BM25\"], [RAG_average_retrieval_time, tfidf_average_retrieval_time,bm25_average_retrieval_time] , width=0.7 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Average Retrieval Time (seconds)\")\n",
    "plt.title(\"Retrieval Time Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySMEO3XBJy3T"
   },
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_retrieval_eval_basic(question, docs_a, docs_b):\n",
    "\n",
    "  sys_prompt = \"\"\"\n",
    "  You are an expert evaluator tasked with comparing two retrieval methods based on how relevant their retrieved documents are to a given question. Only consider document relevance to the question. Ignore writing style or fluency..\n",
    "\n",
    "  Your goal is to determine which method retrieved documents that are more helpful for answering the question, based **only on relevance**.\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### Evaluation Guidelines:\n",
    "\n",
    "  - Assess **only** how relevant the retrieved documents are to the user's question.\n",
    "  - Ignore writing style, formatting, or fluency.\n",
    "  - Consider whether the documents directly contain **useful, specific, or factual information** that would help answer the question.\n",
    "  - Penalize methods that return off-topic, generic, or vague content.\n",
    "  - Favor documents that clearly address the **core intent** of the question.\n",
    "  ---\n",
    "  ### Scoring Instructions:\n",
    "\n",
    "  Carefully compare the two methods. Then respond with:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"better_method\": \"A\" or \"B\",\n",
    "    \"explanation\": \"A brief explanation of why one set of documents was more relevant.\"\n",
    "  }\n",
    "  \"\"\"\n",
    "  answer_grader_prompt = \"\"\"\n",
    "  QUESTION:\n",
    "  {question}\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### Documents retrieved by Method A (RAG-based retrieval):\n",
    "  {docs_a}\n",
    "\n",
    "  ---\n",
    "\n",
    "  ### Documents retrieved by Method B (TF-IDF-based retrieval):\n",
    "  {docs_b}\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  doc_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "      docs_a=docs_a, question=question,docs_b=docs_b\n",
    "  )\n",
    "  result = llm_json_mode.invoke(\n",
    "      [SystemMessage(content=sys_prompt)]\n",
    "      + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "  )\n",
    "\n",
    "  return json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qp-IXviGMRgX",
    "outputId": "38a81345-cdd5-4be3-b808-2185fa92b5de"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "output_file= \"/content/retrieval_comparison_results.txt\"\n",
    "with open(output_file , \"a\" , encoding=\"utf-8\") as f:\n",
    "  for question in tqdm(evaluation_questions, desc=\"Processing Questions\", unit=\"qst\"):\n",
    "    #eval RAG retrieval system\n",
    "    RAG_retrieved_docs = retrieve_context(question)\n",
    "\n",
    "    #eval simple TF-IDF retrieval system\n",
    "    tfidf_retrieved_docs = tfidf_docs_retriever(question)\n",
    "\n",
    "    # Format retrieved docs\n",
    "    RAG_formatted_docs = \"\\n\".join([doc.page_content for doc in RAG_retrieved_docs])\n",
    "    tfidf_formatted_docs = \"\\n\".join([doc.page_content for doc in tfidf_retrieved_docs])\n",
    "\n",
    "    best_method = contrastive_retrieval_eval_basic(question, RAG_formatted_docs, tfidf_formatted_docs)[\"better_method\"]\n",
    "    explanation = contrastive_retrieval_eval_basic(question, RAG_formatted_docs, tfidf_formatted_docs)[\"explanation\"]\n",
    "\n",
    "    results.append(best_method)\n",
    "\n",
    "    #write to the result file\n",
    "    f.write(f\"- Question: {question}\\n\")\n",
    "    f.write(f\"-- Best Method: {best_method}\\n\")\n",
    "    f.write(f\"--- Explanation: {explanation}\\n\")\n",
    "    f.write(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "26pMl3mkO_Fx",
    "outputId": "af6e2adc-2db1-4da1-bafa-ab83aa5aa470"
   },
   "outputs": [],
   "source": [
    "count = Counter(results)\n",
    "\n",
    "RAG_method_count = count['A']\n",
    "TFIDF_method_count = count['B']\n",
    "\n",
    "print(\"Count of RAG method retrieval:\", RAG_method_count)\n",
    "print(\"Count of TFIDF method retrieval:\", TFIDF_method_count)\n",
    "print(\" -------- \")\n",
    "# Bar chart for Best method\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"RAG\", \"TF-IDF\"], [RAG_method_count, TFIDF_method_count],width =0.7 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Number of Times Chosen as Best\")\n",
    "plt.title(\"Comparison of Best Retrieval Method Selection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A02gc6GMkbaP",
    "outputId": "ddf15ff9-6216-4773-e7be-67e7a3ad593b"
   },
   "outputs": [],
   "source": [
    "### Evaluate answers generated by RAG chatbot and other RAGs ###\n",
    "#time\n",
    "RAG_answer_times = []\n",
    "LLM_answer_times = []\n",
    "web_search_answer_times = []\n",
    "#score\n",
    "RAG_answer_score = []\n",
    "LLM_answer_score = []\n",
    "web_seaerch_chatbot_score = []\n",
    "#answers\n",
    "RAGs_answer = []\n",
    "LLM_answer = []\n",
    "web_search_answers = []\n",
    "ALL_web_search_docs = []\n",
    "\n",
    "tfidf_llama_answer = []\n",
    "tfidf_llama_docs = []\n",
    "\n",
    "output_file = \"/content/answer_scores_result.txt\"\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for i in tqdm(range(len(evaluation_questions)), desc=\"Processing Questions\", unit=\"question\"):\n",
    "        retrieved_docs = retrieve_context(evaluation_questions[i])\n",
    "\n",
    "        # Eval RAG chatbot answers\n",
    "        RAG_start_time = time.time()\n",
    "        rag_output, retrieved_docs, x = generate_answer_llama(evaluation_questions[i], retrieved_docs)\n",
    "        RAG_answer_times.append(time.time() - RAG_start_time)\n",
    "        RAGs_answer.append(rag_output) # append all answers to use in ragas\n",
    "\n",
    "        # Eval simple llama3.2 model answers (base model) - no retrieved docs\n",
    "        LLM_start_time = time.time()\n",
    "        simple_llm_response = simple_llama_answer(evaluation_questions[i])\n",
    "        LLM_answer_times.append(time.time() - LLM_start_time)\n",
    "        LLM_answer.append(simple_llm_response) # append all answers to use in ragas\n",
    "\n",
    "        # Eval RAG using web search documents + llama\n",
    "        web_search_start_time = time.time()\n",
    "        web_search_chatbot_response, chatbot_web_search_retr_docs = chatbot_web_search_docs(evaluation_questions[i])\n",
    "        web_search_answer_times.append(time.time() - web_search_start_time)\n",
    "        ALL_web_search_docs.append(chatbot_web_search_retr_docs)\n",
    "        web_search_answers.append(web_search_chatbot_response) # append all answers to use in ragas\n",
    "\n",
    "        # Eval RAG: TF-IDF + llama\n",
    "        simple_llama , simpel_docs = simple_generate_answer_llama(evaluation_questions[i])\n",
    "        tfidf_llama_answer.append(simple_llama)\n",
    "        tfidf_llama_docs.append(simpel_docs)\n",
    "\n",
    "        # Answer grader\n",
    "        RAG_answer = answer_grader(evaluation_questions[i], rag_output, expected_answers[i], retrieved_docs)\n",
    "        llama_answer = answer_grader(evaluation_questions[i], simple_llm_response, expected_answers[i])\n",
    "        web_search_answer = answer_grader(evaluation_questions[i], web_search_chatbot_response, expected_answers[i])\n",
    "\n",
    "        # Write to the result file\n",
    "        f.write(f\"Question: {evaluation_questions[i]}\\n\")\n",
    "        f.write(f\"Generated answer by the Chatbot:\\n{rag_output}\\n\")\n",
    "        f.write(f\"Generated answer by Llama 3.2 model:\\n{simple_llm_response}\\n\")\n",
    "        f.write(f\"Generated answer by the Chatbot using web search:\\n{web_search_chatbot_response}\\n\")\n",
    "        f.write(f\"Expected answer:\\n{expected_answers[i]}\\n\")\n",
    "        f.write(f\"Chatbot answer Score: {RAG_answer}\\n\")\n",
    "        f.write(f\"Llama 3.2 answer Score: {llama_answer}\\n\")\n",
    "        f.write(f\"Chatbot using web search answer Score: {web_search_answer}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")  # Separator\n",
    "\n",
    "        LLM_answer_score.append(llama_answer['answer_score'])\n",
    "        RAG_answer_score.append(RAG_answer['answer_score'])\n",
    "        web_seaerch_chatbot_score.append(web_search_answer['answer_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2RMYW5BC95d"
   },
   "outputs": [],
   "source": [
    "## ragas\n",
    "import pandas as pd\n",
    "#prepare data\n",
    "\n",
    "## first db: rag system answers and retrieved docs\n",
    "RAG_data = [{\"user_input\": query, \"response\": response , \"retrieved_contexts\":[retrieved_d] , \"reference\": ground_truth} for query, response, retrieved_d, ground_truth in zip(evaluation_questions, RAGs_answer,ALL_RAG_retrieved_docs_STRING,expected_answers)]\n",
    "\n",
    "## second db: web search chatbot answers and retrieved docs\n",
    "WEB_search_data = [{\"user_input\": query, \"response\": response , \"retrieved_contexts\":retrieved_d , \"reference\": ground_truth} for query, response, retrieved_d, ground_truth in zip(evaluation_questions, web_search_answers ,ALL_web_search_docs,expected_answers )]\n",
    "\n",
    "## Third db: LLM + TFIDF RAG\n",
    "tfidf_RAG = [{\"user_input\": query, \"response\": response , \"retrieved_contexts\":[retrieved_d] , \"reference\": ground_truth} for query, response, retrieved_d, ground_truth in zip(evaluation_questions, tfidf_llama_answer ,tfidf_llama_docs , expected_answers )]\n",
    "\n",
    "# Fourth db: no retrieved docs Just LLM response\n",
    "LLM_data = [{\"user_input\": query, \"response\": response  , \"reference\": ground_truth} for query, response, ground_truth in zip(evaluation_questions, LLM_answer,expected_answers)]\n",
    "\n",
    "RAG_df = pd.DataFrame(RAG_data)\n",
    "WEB_df = pd.DataFrame(WEB_search_data)\n",
    "tfidf_RAG_df = pd.DataFrame(tfidf_RAG)\n",
    "LLM_data_df = pd.DataFrame(LLM_data)\n",
    "\n",
    "\n",
    "\n",
    "# Write to a JSON file\n",
    "with open(\"/content/RAG_chat_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(RAG_data, f, ensure_ascii=False, indent=4)\n",
    "with open(\"/content/web_search_chat_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(WEB_search_data, f, ensure_ascii=False, indent=4)\n",
    "with open(\"/content/tfidf_RAG_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tfidf_RAG, f, ensure_ascii=False, indent=4)\n",
    "with open(\"/content/LLM_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(LLM_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBqoYnnr2zCC"
   },
   "outputs": [],
   "source": [
    "!ollama pull llama3:8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BTw1djZ21Qa",
    "outputId": "fa74f0ac-7310-40ef-f4bb-7c86edadd9ca"
   },
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJHJ8mtUK9uI"
   },
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "RAG_evaluation_dataset = EvaluationDataset.from_list(RAG_data)\n",
    "tfidf_evaluation_dataset = EvaluationDataset.from_list(tfidf_RAG)\n",
    "WEB_evaluation_dataset = EvaluationDataset.from_list(WEB_search_data)\n",
    "LLM_no_docs_dataset = EvaluationDataset.from_list(LLM_data)\n",
    "\n",
    "llm_ragas = ChatOllama(model=\"llama3:8b\",\n",
    "                           temperature=0.2,\n",
    "                           device_map='cuda')\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (LLMContextRecall, Faithfulness,\n",
    "                           FactualCorrectness,ResponseRelevancy,\n",
    "                           LLMContextPrecisionWithReference,LLMContextPrecisionWithoutReference)\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm_ragas)\n",
    "\n",
    "RAG_result = evaluate(\n",
    "    dataset=RAG_evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(),Faithfulness(),FactualCorrectness(),LLMContextPrecisionWithReference(),LLMContextPrecisionWithoutReference()],\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YImjgr-N92-F"
   },
   "outputs": [],
   "source": [
    "# context_recall\tfaithfulness\tfactual_correctness(mode=f1)\tllm_context_precision_with_reference\tllm_context_precision_without_reference\n",
    "context_recall = RAG_result.to_pandas()['context_recall'].tolist()\n",
    "faithfulness = RAG_result.to_pandas()['faithfulness'].tolist()\n",
    "factual_correctness = RAG_result.to_pandas()['factual_correctness(mode=f1)'].tolist()  # mode='f1' assumed default\n",
    "llm_context_precision_with_reference = RAG_result.to_pandas()['llm_context_precision_with_reference'].tolist()\n",
    "llm_context_precision_without_reference = RAG_result.to_pandas()['llm_context_precision_without_reference'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyuTpHCBMmlP",
    "outputId": "e10cb2fc-fab7-4629-bad3-ebd5fe009094"
   },
   "outputs": [],
   "source": [
    "print(f\"context recall rag : {context_recall}\")\n",
    "print(f\"faithfulness rag : {faithfulness}\")\n",
    "print(f\"factual_correctness rag : {factual_correctness}\")\n",
    "print(f\"llm_context_precision_with_reference rag : {llm_context_precision_with_reference}\")\n",
    "print(f\"llm_context_precision_without_reference rag : {llm_context_precision_without_reference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zF2TECc3fZN"
   },
   "outputs": [],
   "source": [
    "tfidf_RAG_result = evaluate(\n",
    "    dataset=tfidf_evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(),Faithfulness(),FactualCorrectness(),LLMContextPrecisionWithReference(),LLMContextPrecisionWithoutReference()],\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6mRpmTzE2Dn"
   },
   "outputs": [],
   "source": [
    "context_recall_tfidf = tfidf_RAG_result.to_pandas()['context_recall'].tolist()\n",
    "faithfulness_tfidf  = tfidf_RAG_result.to_pandas()['faithfulness'].tolist()\n",
    "factual_correctness_tfidf  = tfidf_RAG_result.to_pandas()['factual_correctness(mode=f1)'].tolist()  # mode='f1' assumed default\n",
    "llm_context_precision_with_reference_tfidf  = tfidf_RAG_result.to_pandas()['llm_context_precision_with_reference'].tolist()\n",
    "llm_context_precision_without_reference_tfidf  = tfidf_RAG_result.to_pandas()['llm_context_precision_without_reference'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VlV5qEjNIk_",
    "outputId": "829bf138-d62a-4254-b0a9-3c71f1267df0"
   },
   "outputs": [],
   "source": [
    "print(\"context_recall_tfidf:\", context_recall_tfidf)\n",
    "print(\"faithfulness_tfidf:\", faithfulness_tfidf)\n",
    "print(\"factual_correctness_tfidf:\", factual_correctness_tfidf)\n",
    "print(\"llm_context_precision_with_reference_tfidf:\", llm_context_precision_with_reference_tfidf)\n",
    "print(\"llm_context_precision_without_reference_tfidf:\", llm_context_precision_without_reference_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4bQOgBj3hsg"
   },
   "outputs": [],
   "source": [
    "WEB_result = evaluate(\n",
    "    dataset=WEB_evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(),Faithfulness(),FactualCorrectness(),LLMContextPrecisionWithReference(),LLMContextPrecisionWithoutReference()],\n",
    "    llm=evaluator_llm,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Him90gLMbX4",
    "outputId": "aa801343-3682-417a-da61-11e57fd8be87"
   },
   "outputs": [],
   "source": [
    "WEB_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0gWLnczNe86",
    "outputId": "ca0b56a1-6975-4dc3-c429-1fa77f676842"
   },
   "outputs": [],
   "source": [
    "# Extract metrics from WEB_result\n",
    "context_recall_web = WEB_result.to_pandas()['context_recall'].tolist()\n",
    "faithfulness_web = WEB_result.to_pandas()['faithfulness'].tolist()\n",
    "factual_correctness_web = WEB_result.to_pandas()['factual_correctness(mode=f1)'].tolist()\n",
    "llm_context_precision_with_reference_web = WEB_result.to_pandas()['llm_context_precision_with_reference'].tolist()\n",
    "llm_context_precision_without_reference_web = WEB_result.to_pandas()['llm_context_precision_without_reference'].tolist()\n",
    "\n",
    "# Print each list\n",
    "print(\"context_recall_web:\", context_recall_web)\n",
    "print(\"faithfulness_web:\", faithfulness_web)\n",
    "print(\"factual_correctness_web:\", factual_correctness_web)\n",
    "print(\"llm_context_precision_with_reference_web:\", llm_context_precision_with_reference_web)\n",
    "print(\"llm_context_precision_without_reference_web:\", llm_context_precision_without_reference_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrZub9VQ3jTC"
   },
   "outputs": [],
   "source": [
    "LLM_no_docs_result = evaluate(\n",
    "    dataset=LLM_no_docs_dataset,\n",
    "    metrics=[FactualCorrectness()],\n",
    "    llm=evaluator_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LKbIQ1qmQL9",
    "outputId": "13a1bd8e-33aa-4ece-a041-76ea49960669"
   },
   "outputs": [],
   "source": [
    "LLM_no_docs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpemPNkENtzL",
    "outputId": "37104054-c0a0-4634-ae33-9c8c2c2fd198"
   },
   "outputs": [],
   "source": [
    "# Extract metrics from LLM_no_docs_result\n",
    "factual_correctness_llm_no_docs = LLM_no_docs_result.to_pandas()['factual_correctness(mode=f1)'].tolist()\n",
    "\n",
    "\n",
    "# Print each list\n",
    "print(\"factual_correctness_llm_no_docs:\", factual_correctness_llm_no_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnKFVhctMvtK",
    "outputId": "f02b51aa-4051-46de-9f14-70f2e0e83b99"
   },
   "outputs": [],
   "source": [
    "# print(f\"RAG Result: {RAG_result}\")\n",
    "# print(\" -------- \")\n",
    "# print(f\"TF-IDF RAG Result: {tfidf_RAG_result}\")\n",
    "# print(\" -------- \")\n",
    "# print(f\"WEB Result: {WEB_result}\")\n",
    "# print(\" -------- \")\n",
    "# print(f\"LLM Result: {LLM_no_docs_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z13TGoJUWAi_",
    "outputId": "de9a91b1-d84f-42ac-a6e0-7168566db813"
   },
   "outputs": [],
   "source": [
    "## CALCULATE EVALUATION METRICS FOR ANSWERING\n",
    "\n",
    "####  CALCULATE AVERAGE ANSWER SCORE:\n",
    "#Chatbot\n",
    "Chatbot_average_answer_score = sum(RAG_answer_score) / len(RAG_answer_score)\n",
    "print(f\"Average Answer Score: {Chatbot_average_answer_score}\")\n",
    "#llama\n",
    "LLM_average_answer_score = sum(LLM_answer_score) / len(LLM_answer_score)\n",
    "print(f\"Average Answer Score: {LLM_average_answer_score}\")\n",
    "#web_search\n",
    "web_search_average_answer_score = sum(web_seaerch_chatbot_score) / len(web_seaerch_chatbot_score)\n",
    "print(f\"Average Answer Score: {web_search_average_answer_score}\")\n",
    "print(\" -------- \")\n",
    "\n",
    "####  CALCULATE ANSWERS SCORE DISTRIBUTION :\n",
    "#Chatbot\n",
    "Chatbot_score_distribution = collections.Counter(RAG_answer_score)\n",
    "print(\"Chatbot score Distribution:\", Chatbot_score_distribution)\n",
    "#llama\n",
    "LLM_score_distribution = collections.Counter(LLM_answer_score)\n",
    "print(\"LLM score Distribution:\", LLM_score_distribution)\n",
    "#web_search\n",
    "web_search_score_distribution = collections.Counter(web_seaerch_chatbot_score)\n",
    "print(\"web_search score Distribution:\", web_search_score_distribution)\n",
    "print(\" -------- \")\n",
    "\n",
    "####  CALCULATE HIT RATE >=4 :\n",
    "#chatbot\n",
    "chatbot_hit_rate = sum(1 for s in RAG_answer_score if s >= 4) / len(RAG_answer_score)\n",
    "print(f\"Chatbot Hit Rate (Score ‚â• 4): {chatbot_hit_rate:.2%}\")\n",
    "#llama\n",
    "llm_hit_rate = sum(1 for s in LLM_answer_score if s >= 4) / len(LLM_answer_score)\n",
    "print(f\"LLM Hit Rate (Score ‚â• 4): {llm_hit_rate:.2%}\")\n",
    "#web_search\n",
    "web_search_hit_rate = sum(1 for s in web_seaerch_chatbot_score if s >= 4) / len(web_seaerch_chatbot_score)\n",
    "print(f\"Chatbot using web search Hit Rate (Score ‚â• 4): {web_search_hit_rate:.2%}\")\n",
    "print(\" -------- \")\n",
    "####  CALCULATE AVERAGE TIME TAKEN FOR ANSWERING :\n",
    "#chatbot\n",
    "chatbot_avg_time = sum(RAG_answer_times) / len(RAG_answer_times)\n",
    "print(f\"Chatbot average Answer Time: {chatbot_avg_time:.2f} seconds\")\n",
    "#llama\n",
    "llm_avg_time = sum(LLM_answer_times) / len(LLM_answer_times)\n",
    "print(f\"LLM average Answer Time: {llm_avg_time:.2f} seconds\")\n",
    "#web_search\n",
    "web_search_avg_time = sum(web_search_answer_times) / len(web_search_answer_times)\n",
    "print(f\"Chatbot using web search average Answer Time: {web_search_avg_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XA2dUNdGalKt",
    "outputId": "1b11989e-9421-4c84-e200-426e0e85cfea"
   },
   "outputs": [],
   "source": [
    "### plot graphs for answer evaluation ###\n",
    "\n",
    "# Bar chart for average answer scores\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Chatbot (RAG)\", \"LLM\", \"Web search chatbot\"], [Chatbot_average_answer_score, LLM_average_answer_score,web_search_average_answer_score], width=0.5, edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Average Answer Score\")\n",
    "plt.title(\"Average Answer Score Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for hit rates\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Chatbot (RAG)\", \"LLM\",\"Web search chatbot\"], [chatbot_hit_rate, llm_hit_rate,web_search_hit_rate] , width=0.5 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Hit Rate (Score ‚â• 4)\")\n",
    "plt.title(\"Hit Rate Comparison (Answering)\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for average answering times\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Chatbot (RAG)\", \"LLM\",\"Web search chatbot\"], [chatbot_avg_time, llm_avg_time,web_search_avg_time], width=0.5 , edgecolor=\"white\", linewidth=0.7)\n",
    "plt.ylabel(\"Average Answer Time (seconds)\")\n",
    "plt.title(\"Answering Time Comparison\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBKs6D6zq3b-",
    "outputId": "acf5baa0-aff4-4579-c8f1-2acc78320be7"
   },
   "outputs": [],
   "source": [
    "### Evaluate answers generated by RAG chatbot, simple llm, and a web search based chatbot, based on \"coherence\",\"relevance\", and \"fluency\" ###\n",
    "\n",
    "\n",
    "RAG_answer_coherence = []\n",
    "RAG_answer_relevance = []\n",
    "RAG_answer_fluency = []\n",
    "llm_answer_coherence = []\n",
    "llm_answer_relevance = []\n",
    "llm_answer_fluency = []\n",
    "web_search_answer_coherence = []\n",
    "web_search_answer_relevance = []\n",
    "web_search_answer_fluency = []\n",
    "\n",
    "output_file = \"/content/chatbot_quality_score.txt\"\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for i in tqdm(range(len(evaluation_questions)), desc=\"Processing Questions\", unit=\"question\"):\n",
    "\n",
    "        # Eval RAG chatbot answers\n",
    "        retrieved_docs = retrieve_context(evaluation_questions[i])\n",
    "        rag_output, retrieved_docs,x = generate_answer_llama(evaluation_questions[i], retrieved_docs)\n",
    "\n",
    "        # Eval simple llama3.2 model answers (base model)\n",
    "        simple_llm_response = simple_llama_answer(evaluation_questions[i])\n",
    "\n",
    "        # Eval web search chatbot\n",
    "        web_search_chatbot_response = chatbot_web_search_docs(evaluation_questions[i])\n",
    "\n",
    "\n",
    "        RAG_answer_quality = response_quality_evaluator(evaluation_questions[i], rag_output, expected_answers[i])\n",
    "        llama_answer_quality = response_quality_evaluator(evaluation_questions[i], simple_llm_response, expected_answers[i])\n",
    "        web_search_answer_quality = response_quality_evaluator(evaluation_questions[i], web_search_chatbot_response, expected_answers[i])\n",
    "\n",
    "        # Write to the result file\n",
    "        f.write(f\"Question: {evaluation_questions[i]}\\n\")\n",
    "        f.write(f\"---> Chatbot answer quality score : {RAG_answer_quality}\\n\")\n",
    "        f.write(f\"---> LLM answer quality score : {llama_answer_quality}\\n\")\n",
    "        f.write(f\"---> Chatbot using web search answer quality score : {web_search_answer_quality}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")  # Separator\n",
    "\n",
    "        RAG_answer_coherence.append(RAG_answer_quality['coherence'])\n",
    "        RAG_answer_relevance.append(RAG_answer_quality['relevance'])\n",
    "        RAG_answer_fluency.append(RAG_answer_quality['fluency'])\n",
    "        llm_answer_coherence.append(llama_answer_quality['coherence'])\n",
    "        llm_answer_relevance.append(llama_answer_quality['relevance'])\n",
    "        llm_answer_fluency.append(llama_answer_quality['fluency'])\n",
    "        web_search_answer_coherence.append(web_search_answer_quality['coherence'])\n",
    "        web_search_answer_relevance.append(web_search_answer_quality['relevance'])\n",
    "        web_search_answer_fluency.append(web_search_answer_quality['fluency'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laz5BDs0uXGh",
    "outputId": "d6eeda59-a05c-443c-9c50-423ac1230adc"
   },
   "outputs": [],
   "source": [
    "# Calculate the average coherence, relevance, and fluency scores\n",
    "rag_average_coherence = sum(RAG_answer_coherence) / len(RAG_answer_coherence)\n",
    "rag_average_relevance = sum(RAG_answer_relevance) / len(RAG_answer_relevance)\n",
    "rag_average_fluency = sum(RAG_answer_fluency) / len(RAG_answer_fluency)\n",
    "llm_average_coherence = sum(llm_answer_coherence) / len(llm_answer_coherence)\n",
    "llm_average_relevance = sum(llm_answer_relevance) / len(llm_answer_relevance)\n",
    "llm_average_fluency = sum(llm_answer_fluency) / len(llm_answer_fluency)\n",
    "web_search_avg_coherence = sum(web_search_answer_coherence) / len(web_search_answer_coherence)\n",
    "web_search_avg_relevance = sum(web_search_answer_relevance) / len(web_search_answer_relevance)\n",
    "web_search_avg_fluency = sum(web_search_answer_fluency) / len(web_search_answer_fluency)\n",
    "\n",
    "\n",
    "print(f\"RAG Average Coherence: {rag_average_coherence:.2f}\")\n",
    "print(f\"LLM Average Coherence: {llm_average_coherence:.2f}\")\n",
    "print(f\"Chatbot using web search Average Coherence: {web_search_avg_coherence:.2f}\\n\")\n",
    "print(\" -------- \")\n",
    "print(f\"RAG Average Relevance: {rag_average_relevance:.2f}\")\n",
    "print(f\"LLM Average Relevance: {llm_average_relevance:.2f}\")\n",
    "print(f\"Chatbot using web search Average Relevance: {web_search_avg_relevance:.2f}\\n\")\n",
    "print(\" -------- \")\n",
    "print(f\"RAG Average Fluency: {rag_average_fluency:.2f}\")\n",
    "print(f\"LLM Average Fluency: {llm_average_fluency:.2f}\")\n",
    "print(f\"Chatbot using web search Average Fluency: {web_search_avg_fluency:.2f}\\n\")\n",
    "\n",
    "# Calculate Hit Rate (Score ‚â• 4) for each metric\n",
    "rag_hit_rate_coherence = sum(1 for s in RAG_answer_coherence if s >= 4) / len(RAG_answer_coherence)\n",
    "rag_hit_rate_relevance = sum(1 for s in RAG_answer_relevance if s >= 4) / len(RAG_answer_relevance)\n",
    "rag_hit_rate_fluency = sum(1 for s in RAG_answer_fluency if s >= 4) / len(RAG_answer_fluency)\n",
    "llm_hit_rate_coherence = sum(1 for s in llm_answer_coherence if s >= 4) / len(llm_answer_coherence)\n",
    "llm_hit_rate_relevance = sum(1 for s in llm_answer_relevance if s >= 4) / len(llm_answer_relevance)\n",
    "llm_hit_rate_fluency = sum(1 for s in llm_answer_fluency if s >= 4) / len(llm_answer_fluency)\n",
    "web_search_hit_rate_coherence = sum(1 for s in web_search_answer_coherence if s >= 4) / len(web_search_answer_coherence)\n",
    "web_search_hit_rate_relevance = sum(1 for s in web_search_answer_relevance if s >= 4) / len(web_search_answer_relevance)\n",
    "web_search_hit_rate_fluency = sum(1 for s in web_search_answer_fluency if s >= 4) / len(web_search_answer_fluency)\n",
    "print(\" -------- \")\n",
    "print(f\"RAG Hit Rate (Score ‚â• 4) - Coherence: {rag_hit_rate_coherence:.2%}\")\n",
    "print(f\"LLM Hit Rate (Score ‚â• 4) - Coherence: {llm_hit_rate_coherence:.2%}\")\n",
    "print(f\"Chatbot using web search Hit Rate (Score ‚â• 4) - Coherence: {web_search_hit_rate_coherence:.2%}\\n\")\n",
    "print(\" -------- \")\n",
    "print(f\"RAG Hit Rate (Score ‚â• 4) - Relevance: {rag_hit_rate_relevance:.2%}\")\n",
    "print(f\"LLM Hit Rate (Score ‚â• 4) - Relevance: {llm_hit_rate_relevance:.2%}\")\n",
    "print(f\"Chatbot using web search Hit Rate (Score ‚â• 4) - Relevance: {web_search_hit_rate_relevance:.2%}\\n\")\n",
    "print(\" -------- \")\n",
    "print(f\"RAG Hit Rate (Score ‚â• 4) - Fluency: {rag_hit_rate_fluency:.2%}\")\n",
    "print(f\"LLM Hit Rate (Score ‚â• 4) - Fluency: {llm_hit_rate_fluency:.2%}\")\n",
    "print(f\"Chatbot using web search Hit Rate (Score ‚â• 4) - Fluency: {web_search_hit_rate_fluency:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919
    },
    "id": "jl65Qlet_O1c",
    "outputId": "cbfc29c4-fb79-4a55-b609-ea9894b52ecd"
   },
   "outputs": [],
   "source": [
    "# Define categories and corresponding scores\n",
    "categories = [\"Coherence\", \"Relevance\", \"Fluency\"]\n",
    "models = [\"RAG\", \"LLM\", \"Web Search\"]\n",
    "\n",
    "# Average Scores for each model\n",
    "average_scores = np.array([\n",
    "    [rag_average_coherence, rag_average_relevance, rag_average_fluency],\n",
    "    [llm_average_coherence, llm_average_relevance, llm_average_fluency],\n",
    "    [web_search_avg_coherence, web_search_avg_relevance, web_search_avg_fluency]\n",
    "])\n",
    "\n",
    "# Hit Rates for each model\n",
    "hit_rates = np.array([\n",
    "    [rag_hit_rate_coherence, rag_hit_rate_relevance, rag_hit_rate_fluency],\n",
    "    [llm_hit_rate_coherence, llm_hit_rate_relevance, llm_hit_rate_fluency],\n",
    "    [web_search_hit_rate_coherence, web_search_hit_rate_relevance, web_search_hit_rate_fluency]\n",
    "])\n",
    "\n",
    "# Define bar positions\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25  # Width of bars\n",
    "\n",
    "# Plot Average Answer Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width, average_scores[0], width, label=\"RAG\", color='blue', alpha=0.7)\n",
    "plt.bar(x, average_scores[1], width, label=\"LLM\", color='black', alpha=0.7)\n",
    "plt.bar(x + width, average_scores[2], width, label=\"Web Search\", color='red', alpha=0.7)\n",
    "\n",
    "plt.xticks(x, categories)\n",
    "plt.ylabel(\"Average Score\")\n",
    "plt.title(\"Comparison of Average Answer Scores\")\n",
    "plt.ylim(0, 5)  # Assuming scores range from 1 to 5\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Hit Rates\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width, hit_rates[0], width, label=\"RAG\", color='blue', alpha=0.7)\n",
    "plt.bar(x, hit_rates[1], width, label=\"LLM\", color='black', alpha=0.7)\n",
    "plt.bar(x + width, hit_rates[2], width, label=\"Web Search\", color='red', alpha=0.7)\n",
    "\n",
    "plt.xticks(x, categories)\n",
    "plt.ylabel(\"Hit Rate (Score ‚â• 4)\")\n",
    "plt.title(\"Comparison of Hit Rates for Answer Quality\")\n",
    "plt.ylim(0, 1)  # Hit rates range from 0 to 1 (percentage format in previous print output)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TP58TeYS_3V"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_hVshAsrZ8Q",
    "outputId": "05d3980e-423d-4c1c-fcbe-c80e55afe017"
   },
   "outputs": [],
   "source": [
    "### Evaluate answers by RAG chatbot and simple llm bassed on hallucination ###\n",
    "\n",
    "RAG_hallucination_score = []\n",
    "LLM_halllucination_score = []\n",
    "Web_Search_hallucination_score = []\n",
    "\n",
    "output_file = \"/content/hallucination_score.txt\"\n",
    "with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "    for i in tqdm(range(len(evaluation_questions)), desc=\"Processing Questions\", unit=\"question\"):\n",
    "\n",
    "        # Eval RAG chatbot answers\n",
    "        retrieved_docs = retrieve_context(evaluation_questions[i])\n",
    "        rag_output, retrieved_docs,x = generate_answer_llama(evaluation_questions[i], retrieved_docs)\n",
    "\n",
    "        # Eval simple llama3.2 model answers (base model)\n",
    "        simple_llm_response = simple_llama_answer(evaluation_questions[i])\n",
    "\n",
    "        # Eval web search chatbot\n",
    "        web_search_chatbot_response = chatbot_web_search_docs(evaluation_questions[i])\n",
    "\n",
    "        RAG_hallucination = hallucination_grader(evaluation_questions[i],retrieved_docs, rag_output)\n",
    "        llama_hallucination = hallucination_grader(evaluation_questions[i],retrieved_docs, simple_llm_response)\n",
    "        web_search_hallucination = hallucination_grader(evaluation_questions[i],retrieved_docs, web_search_chatbot_response)\n",
    "\n",
    "        # Write to the result file\n",
    "        f.write(f\"Question: {evaluation_questions[i]}\\n\")\n",
    "        f.write(f\"---> Chatbot generated answer: {rag_output}\\n\")\n",
    "        f.write(f\"Chatbot answer hallucination score : {RAG_hallucination['hallucination_score']}\\n\")\n",
    "        f.write(f\"Chatbot answer hallucination explanation : {RAG_hallucination['explanation']}\\n\")\n",
    "        f.write(f\"---> LLM genrated answer: {simple_llm_response}\\n\")\n",
    "        f.write(f\"LLM answer hallucination score : {llama_hallucination['hallucination_score']}\\n\")\n",
    "        f.write(f\"LLM answer hallucination explanation : {llama_hallucination['explanation']}\\n\")\n",
    "        f.write(f\"---> Chatbot using web search generated answer: {web_search_chatbot_response}\\n\")\n",
    "        f.write(f\"Chatbot using web search answer hallucination score : {web_search_hallucination['hallucination_score']}\\n\")\n",
    "        f.write(f\"Chatbot using web search answer hallucination explanation : {web_search_hallucination['explanation']}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\")  # Separator\n",
    "\n",
    "        RAG_hallucination_score.append(RAG_hallucination['hallucination_score'])\n",
    "        LLM_halllucination_score.append(llama_hallucination['hallucination_score'])\n",
    "        Web_Search_hallucination_score.append(web_search_hallucination['hallucination_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wx3EPJ7h7Kf3",
    "outputId": "9c97fd40-b648-4c77-f92e-05c546671259"
   },
   "outputs": [],
   "source": [
    "## CALCULATE EVALUATION METRICS FOR ANSWERS HALLUCINATION\n",
    "#Average Score\n",
    "rag_average_hallucination_score = sum(RAG_hallucination_score) / len(RAG_hallucination_score)\n",
    "llm_average_hallucination_score = sum(LLM_halllucination_score) / len(LLM_halllucination_score)\n",
    "web_search_average_hallucination_score = sum(Web_Search_hallucination_score) / len(Web_Search_hallucination_score)\n",
    "print(f\"RAG Average Hallucination Score: {rag_average_hallucination_score}\")\n",
    "print(f\"LLM Average Hallucination Score: {llm_average_hallucination_score}\")\n",
    "print(f\"Chatbot using web search Average Hallucination Score: {web_search_average_hallucination_score}\\n\")\n",
    "print(\" ----- \")\n",
    "# Score Distribution\n",
    "rag_hallucination_distribution = collections.Counter(RAG_hallucination_score)\n",
    "print(\"\\nChatbot score Distribution:\", rag_hallucination_distribution)\n",
    "llm_hallucination_distribution = collections.Counter(LLM_halllucination_score)\n",
    "print(\"LLM score Distribution:\", llm_hallucination_distribution)\n",
    "web_search_hallucination_distribution = collections.Counter(Web_Search_hallucination_score)\n",
    "print(\"Chatbot using web search score Distribution:\", web_search_hallucination_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9zHouH1o9S8"
   },
   "outputs": [],
   "source": [
    "models = [\"RAG\", \"LLM\", \"Web Search\"]\n",
    "\n",
    "# Average Hallucination Scores\n",
    "average_hallucination_scores = [\n",
    "    rag_average_hallucination_score,\n",
    "    llm_average_hallucination_score,\n",
    "    web_search_average_hallucination_score\n",
    "]\n",
    "\n",
    "# Plot Average Hallucination Scores (Bar Chart)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, average_hallucination_scores, color=['blue', 'black', 'red'], alpha=0.7,width=0.5)\n",
    "plt.ylabel(\"Average Hallucination Score\")\n",
    "plt.title(\"Comparison of Average Hallucination Scores (3:BEST)\")\n",
    "plt.ylim(0, max(average_hallucination_scores) + 0.5)\n",
    "plt.show()\n",
    "\n",
    "# Score Distribution - Ensure consistent x-axis values across all models\n",
    "all_scores = sorted(set(rag_hallucination_distribution.keys()) |\n",
    "                    set(llm_hallucination_distribution.keys()) |\n",
    "                    set(web_search_hallucination_distribution.keys()))\n",
    "\n",
    "rag_values = [rag_hallucination_distribution.get(score, 1) for score in all_scores]\n",
    "llm_values = [llm_hallucination_distribution.get(score, 1) for score in all_scores]\n",
    "web_values = [web_search_hallucination_distribution.get(score, 1) for score in all_scores]\n",
    "\n",
    "# Bar plot for hallucination score distribution\n",
    "x = np.arange(len(all_scores))  # X-axis positions\n",
    "width = 0.25  # Width of bars\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - width, rag_values, width, label=\"RAG\", color='blue', alpha=0.7)\n",
    "plt.bar(x, llm_values, width, label=\"LLM\", color='black', alpha=0.7)\n",
    "plt.bar(x + width, web_values, width, label=\"Web Search\", color='red', alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Hallucination Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Hallucination Scores Across Models\")\n",
    "plt.xticks(x, all_scores)  # Set x-axis labels\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_zUAvDmuq89"
   },
   "source": [
    "---\n",
    "\n",
    "## üèÜ **Key Findings**\n",
    "- The **RAG chatbot consistently outperforms baseline retrieval methods (TF-IDF, BM25)** in **relevance and hit rate**.\n",
    "- **LLM-only answers lack factual accuracy** compared to **RAG-based answers**.\n",
    "- **Web search retrieval can introduce off-topic information**, reducing coherence and relevance.\n",
    "- The **hallucination rate is lowest in the RAG chatbot**, confirming that retrieval-grounded responses are **more reliable**.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• **Future Improvements**\n",
    "üîπ **Improve retrieval ranking** with **cross-encoder reranking** and other new techniques and technologies.   \n",
    "üîπ **Enhance factual grounding** using **fact-checking modules**.  \n",
    "üîπ Add **PDF input** Feature for users.  \n",
    "üîπ Incorporating **agentic AI principles** using frameworks like LangGraph.<br>\n",
    "üîπ Question Reformulation.   \n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ **Conclusion**\n",
    "This evaluation framework enables a **deep and systematic analysis** of both retrieval quality and generation performance across RAG and baseline models.  \n",
    "It highlights the strengths of **retrieval-augmented generation**, especially for applications requiring factual accuracy and relevance, and provides actionable insights for future development.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
